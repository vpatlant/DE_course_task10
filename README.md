# Итоговое задание №10

### Задание:

Вы работаете в компании, которая управляет крупным интернет-магазином. Ваша задача — создать автоматизированный пайплайн обработки и анализа данных о продажах, используя стек технологий: PostgreSQL, ClickHouse, Apache Airflow и PySpark. Пайплайн должен выполнять генерацию реалистичных данных о продажах, их обработку, очистку, загрузку в базы данных, а также выполнение аналитических операций.

<b>Обратите внимание, что в проекте используются PySpark, Clickhouse, PostgreSQL. PySpark в airflow в docker-compose нету! Его необходимо добавить самостоятельно. Куда вставить в docker-compose - задача самостоятельная. </b>

Условие задачи следующее - 

* Сгенерировать данные о 1 миллионе продаж за последний год.
* Каждая запись о продаже должна содержать следующие поля:
- sale_id (уникальный идентификатор продажи).
- customer_id (идентификатор клиента).
- product_id (идентификатор продукта).
- quantity (количество купленных товаров).
- sale_date (дата продажи).
- sale_amount (сумма продажи, рассчитывается как количество товаров * случайная цена товара).
- region (регион клиента, один из: North, South, East, West).

* Удалить дубликаты записей о продажах.
* Привести данные к нужным форматам для дальнейшей обработки.
* Создать таблицу для хранения данных о продажах в PostgreSQL.
* Вставить очищенные данные в эту таблицу.
* Выполнить агрегацию данных, используя оконные функции и группировки:
- Подсчитать общее количество продаж и сумму продаж для каждого региона и каждого продукта.
- Рассчитать средний чек (average_sale_amount) по регионам и продуктам.
- Сохранить агрегированные данные в отдельную таблицу в PostgreSQL.
* Перенести агрегированные данные из PostgreSQL в ClickHouse. Добавить дату импорта. 
<b>Обращаем Ваше внимание на то, что все операции по созданию таблиц, обработке, миграции данных должны происходить внутри Airflow. Ничего в Dbeaver делать не нужно. Расписание DAG -  12:45 по Москве, каждый вторник.</b>


### Решение


<b>Рабочая среда</b>

- Скачиваем предлагаемый [yml файл](https://github.com/Artemka5240/final_docker_compose/blob/main/docker-compose.yml) файл.
- Модифицируем образ apache/airflow интегрировав в него Java и необходимые [библиотеки](https://github.com/vpatlant/DE_course_task10/blob/main/DWH/requirements.txt), для этого добавим [Docker]{https://github.com/vpatlant/DE_course_task10/blob/main/DWH/Dockerfile} файл в директорию проекта.
- Внесем [правки](https://github.com/vpatlant/DE_course_task10/blob/main/DWH/docker-compose.yml) в yaml файл


<b>Генератор</b>
Для генерации более реалистичных данных воспользуемся открытыми данными:
"По итогам 2023 года активных покупателей на маркетплейсе стало больше на треть – 46,1 млн человек против 35,2 млн годом ранее. Лояльность аудитории существенно выросла: каждый активный покупатель в среднем заказывает на ... 21 раз в год по сравнению с 13 заказами в 2022 году" 

Масштабируя данную статистику к условиям задачи выставим начальное количество пользователей START_PERSON_COUNT = 72700, финальное -FINAL_PERSON_COUNT = 142900

При генерации, пользователей будем соотносить с одним из требуемых регионов (North, South, East, West). 
Для реалистичности:
 - выберем 4 региона РФ
 - спарсим некоторое количество адресов из выбранных регионов
 - спарсим некоторое количество ФИО людей из выбранных регионов (для соблюдения закона о персональных данных, ФИО разделим на составляющие и перемешаем) <b>все персонажи и события вымышлены, любые совпадения случайны и блаблабла</b>
 - возраст MAX_PERSON_AGE не более 87 лет
 - дата регистрации с момента "запуска" магазина STORE_START_DATE = 2023-01-01 до дня формирования отчета
 - возраст на момент регистрации > 18 лет
 
Для генерации реалистичных товарных позиций, спарсим некоторое количество товарных позиций одной из торговых сетей.
 
Спаршенные данные выложим в виде [гугл-таблицы](https://docs.google.com/spreadsheets/d/1DMKQwUrU2OJNpsK0GJhK7pB7OJ-tRwUdVWmAyjAFDYM/edit?gid=1408946552#gid=1408946552) и при генерации будем извлекать данные из неё.
 
Так как, целью задачи является разработка реалистичного ETL-процесса с запуском еженедельно, то данные будем генерировать так же понедельно, разделяя по регионам.
Примем период генерации данных с ANALYTICS_START_DATE = 2023-09-01 по ANALYTICS_END_DATE = 2024-09-01.
 
Скрипт должен сгенерировать стартовое количество пользователей в первую неделю, далее будем генерировать по (FINAL_PERSON_COUNT - START_PERSON_COUNT) // (365 / 7) пользователей с погрешностью */-10%.
Для пользователей введем параметр активности - как показатель максимального количества возможных заказов, таким образом получится, что часть пользователей будет неактивной.
Количество заказов еженедельно (кроме последней недели): orders_count_mean = 1000000 // (365 / 7) +/- 10%. Для последней недели - оставшееся количество заказов до 1000000.
 
С целью отработки внештатных ситуаций с возможным отсутствием данных запланируем "пропажу" данных по одному из регионов с 20й недели.
 
 
<b>ETL</b>
 
 ![dag graph](https://github.com/vpatlant/DE_course_task10/blob/main/img/dag_graph.jpg?raw=true)
 
 1. start  - точка входа
 2. generator - генерация данных за предыдущую неделю
 3. create_agg - проверка существования таблицы аггрегатов в postgres (создание при отсутствии)
 4. clear_agg - удаление данных за прошлую неделю (для ситуаций повторного запуска скрипта через clear)
 5. create_click_agg - проверка существования таблицы аггрегатов в clickhouse (создание при отсутствии)
 6. clear_click_orders_agg - удаление данных за прошлую неделю (для ситуаций повторного запуска скрипта через clear)
 7. copy_aggregates - перенос аггрегатов из postres в clickhouse
 8. finish - точка выхода
 
 при сбое отправка уведомления в Tg
 

 <b>Результат</b>
 скрипт отработал
 
 ![dag graph](https://github.com/vpatlant/DE_course_task10/blob/main/img/execution.jpg?raw=true)
 
 даже при отсутствующих файлах по одному из регионов за 20-23 неделю
 
![dag graph](https://github.com/vpatlant/DE_course_task10/blob/main/img/missing.jpg?raw=true)

[сгенерированные файлы](https://disk.yandex.ru/d/8d_U0FY8dYyReQ)


<b>Визуализируем данные</b>

![dag graph](https://github.com/vpatlant/DE_course_task10/blob/main/img/report.jpg?raw=true)

