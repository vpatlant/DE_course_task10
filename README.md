# Итоговое задание №6

### Задание:

Вы работаете в компании, которая управляет крупным интернет-магазином. Ваша задача — создать автоматизированный пайплайн обработки и анализа данных о продажах, используя стек технологий: PostgreSQL, ClickHouse, Apache Airflow и PySpark. Пайплайн должен выполнять генерацию реалистичных данных о продажах, их обработку, очистку, загрузку в базы данных, а также выполнение аналитических операций.

<b>Обратите внимание, что в проекте используются PySpark, Clickhouse, PostgreSQL. PySpark в airflow в docker-compose нету! Его необходимо добавить самостоятельно. Куда вставить в docker-compose - задача самостоятельная. <b>

Условие задачи следующее - 

* Сгенерировать данные о 1 миллионе продаж за последний год.
* Каждая запись о продаже должна содержать следующие поля:
- sale_id (уникальный идентификатор продажи).
- customer_id (идентификатор клиента).
- product_id (идентификатор продукта).
- quantity (количество купленных товаров).
- sale_date (дата продажи).
- sale_amount (сумма продажи, рассчитывается как количество товаров * случайная цена товара).
- region (регион клиента, один из: North, South, East, West).

* Удалить дубликаты записей о продажах.
* Привести данные к нужным форматам для дальнейшей обработки.
* Создать таблицу для хранения данных о продажах в PostgreSQL.
* Вставить очищенные данные в эту таблицу.
* Выполнить агрегацию данных, используя оконные функции и группировки:
- Подсчитать общее количество продаж и сумму продаж для каждого региона и каждого продукта.
- Рассчитать средний чек (average_sale_amount) по регионам и продуктам.
- Сохранить агрегированные данные в отдельную таблицу в PostgreSQL.
* Перенести агрегированные данные из PostgreSQL в ClickHouse. Добавить дату импорта. 
<b>Обращаем Ваше внимание на то, что все операции по созданию таблиц, обработке, миграции данных должны происходить внутри Airflow. Ничего в Dbeaver делать не нужно. Расписание DAG -  12:45 по Москве, каждый вторник.<b>